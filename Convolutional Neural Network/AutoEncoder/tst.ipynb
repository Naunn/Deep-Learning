{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoEncoder\n",
    "# https://medium.com/analytics-vidhya/image-anomaly-detection-using-autoencoders-ae937c7fd2d1\n",
    "\n",
    "# ogólne\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# ładowanie danych\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# budowanie modelu\n",
    "from keras.layers import LeakyReLU, Conv2D, BatchNormalization, Conv2DTranspose\n",
    "from keras.optimizers import Adam\n",
    "from keras import Model, Input\n",
    "\n",
    "# proces uczenia\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from datetime import datetime\n",
    "\n",
    "# wizualizacja\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import load_img\n",
    "import pickle\n",
    "\n",
    "# klasyfikacja\n",
    "from keras.models import load_model\n",
    "\n",
    "# https://stackoverflow.com/questions/58352326/running-the-tensorflow-2-0-code-gives-valueerror-tf-function-decorated-functio\n",
    "tf.config.run_functions_eagerly(True)  \n",
    "# tf.data.experimental.enable_debug_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 890 images belonging to 1 classes.\n",
      "Found 157 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Przygotowanie danych\n",
    "# https://stackoverflow.com/questions/42443936/keras-split-train-test-set-when-using-imagedatagenerator\n",
    "\n",
    "path = r\"C:\\Users\\User\\Desktop\\Matematyka stosowana - II stopień\\III semestr\\Uczenie ze wzmocnieniem i deeplearning\\deep_learning\\Zadanie 1\"\n",
    "nbatch = 64\n",
    "\n",
    "datagen = ImageDataGenerator(validation_split=0.15,\n",
    "                             rescale=1./255,\n",
    "                             rotation_range=10.,\n",
    "                             width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             zoom_range=0.2,\n",
    "                            #  data_format='channels_last',\n",
    "                             horizontal_flip=True)\n",
    "\n",
    "train_data = datagen.flow_from_directory(directory=path+r\"\\train\",\n",
    "                                         target_size=(224,224),\n",
    "                                         shuffle=True,\n",
    "                                         color_mode=\"grayscale\",\n",
    "                                         batch_size=nbatch,\n",
    "                                         class_mode=\"input\",\n",
    "                                        #  classes=None,\n",
    "                                         subset='training') # set as training data\n",
    "\n",
    "\n",
    "valid_data = datagen.flow_from_directory(directory=path+r\"\\train\",\n",
    "                                         target_size=(224,224),\n",
    "                                         shuffle=True,\n",
    "                                         color_mode=\"grayscale\",\n",
    "                                         batch_size=nbatch,\n",
    "                                         class_mode=\"input\",\n",
    "                                        #  classes=None,\n",
    "                                         subset='validation') # set as validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " conv_1 (Conv2D)             (None, 112, 112, 32)      896       \n",
      "                                                                 \n",
      " batchnorm_1 (BatchNormaliza  (None, 112, 112, 32)     128       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " leaky_relu_1 (LeakyReLU)    (None, 112, 112, 32)      0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,024\n",
      "Trainable params: 960\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape=(224, 224, 3), name='input_layer')\n",
    "\n",
    "# Conv Block 1 -> BatchNorm->leaky Relu\n",
    "encoded = Conv2D(filters=32, kernel_size=(3,3), strides=2, padding='same', name='conv_1')(inputs)\n",
    "encoded = BatchNormalization(name='batchnorm_1')(encoded)\n",
    "encoded = LeakyReLU(name='leaky_relu_1')(encoded)\n",
    "# Conv Block 2 -> BatchNorm->leaky Relu\n",
    "# encoded = Conv2D(filters=64, kernel_size=(3,3), strides=2, padding='same', name='conv_2')(encoded)\n",
    "# encoded = BatchNormalization(name='batchnorm_2')(encoded)\n",
    "# encoded = LeakyReLU(name='leaky_relu_2')(encoded)\n",
    "# # Conv Block 3 -> BatchNorm->leaky Relu\n",
    "# encoded = Conv2D(filters=64, kernel_size=(3,3), strides=(2,2), padding='same', name='conv_3')(encoded)\n",
    "# encoded = BatchNormalization(name='batchnorm_3')(encoded)\n",
    "# encoded = LeakyReLU(name='leaky_relu_3')(encoded)\n",
    "\n",
    "encoder = Model(inputs, encoded)\n",
    "\n",
    "def SSIMLoss(y_true, y_pred):\n",
    "  return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))\n",
    "encoder.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = 0.0005), loss=[SSIMLoss])\n",
    "\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_83696\\720978468.py\", line 19, in SSIMLoss  *\n        return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))\n\n    ValueError: Shapes (224, 224, 3) and (112, 112, 32) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [33], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hist\u001b[39m=\u001b[39mencoder\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[0;32m      2\u001b[0m                  epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m      3\u001b[0m                  batch_size\u001b[39m=\u001b[39;49mnbatch,\n\u001b[0;32m      4\u001b[0m                  shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      5\u001b[0m                  validation_data\u001b[39m=\u001b[39;49mvalid_data)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filey77119sr.py:12\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__SSIMLoss\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m     retval_ \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mreduce_mean, (ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(tf)\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mssim, (ag__\u001b[39m.\u001b[39mld(y_true), ag__\u001b[39m.\u001b[39mld(y_pred), \u001b[39m1.0\u001b[39m), \u001b[39mNone\u001b[39;00m, fscope),), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     13\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_83696\\720978468.py\", line 19, in SSIMLoss  *\n        return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 1.0))\n\n    ValueError: Shapes (224, 224, 3) and (112, 112, 32) are incompatible\n"
     ]
    }
   ],
   "source": [
    "hist=encoder.fit(x=train_data,\n",
    "                 epochs=10,\n",
    "                 batch_size=nbatch,\n",
    "                 shuffle=True,\n",
    "                 validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 224, 224, 1)]     0         \n",
      "                                                                 \n",
      " conv_1 (Conv2D)             (None, 224, 224, 32)      320       \n",
      "                                                                 \n",
      " batchnorm_1 (BatchNormaliza  (None, 224, 224, 32)     128       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " leaky_relu_1 (LeakyReLU)    (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " conv_2 (Conv2D)             (None, 112, 112, 64)      18496     \n",
      "                                                                 \n",
      " batchnorm_2 (BatchNormaliza  (None, 112, 112, 64)     256       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " leaky_relu_2 (LeakyReLU)    (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv_3 (Conv2D)             (None, 56, 56, 64)        36928     \n",
      "                                                                 \n",
      " batchnorm_3 (BatchNormaliza  (None, 56, 56, 64)       256       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " leaky_relu_3 (LeakyReLU)    (None, 56, 56, 64)        0         \n",
      "                                                                 \n",
      " conv_transpose_1 (Conv2DTra  (None, 56, 56, 64)       36928     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batchnorm_4 (BatchNormaliza  (None, 56, 56, 64)       256       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " leaky_relu_4 (LeakyReLU)    (None, 56, 56, 64)        0         \n",
      "                                                                 \n",
      " conv_transpose_2 (Conv2DTra  (None, 112, 112, 64)     36928     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batchnorm_5 (BatchNormaliza  (None, 112, 112, 64)     256       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " leaky_relu_5 (LeakyReLU)    (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " conv_transpose_3 (Conv2DTra  (None, 224, 224, 32)     18464     \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      " batchnorm_6 (BatchNormaliza  (None, 224, 224, 32)     128       \n",
      " tion)                                                           \n",
      "                                                                 \n",
      " leaky_relu_6 (LeakyReLU)    (None, 224, 224, 32)      0         \n",
      "                                                                 \n",
      " conv_transpose_4 (Conv2DTra  (None, 224, 224, 1)      289       \n",
      " nspose)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 149,633\n",
      "Trainable params: 148,993\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the Encoder and Decoder\n",
    "#pass the gray scale input image of size(28,28,1)\n",
    "inputs = Input(shape=(224, 224, 1), name='input_layer')\n",
    "# Conv Block 1 -> BatchNorm->leaky Relu\n",
    "encoded = Conv2D(32, kernel_size=(3,3), strides= 1, padding='same', name='conv_1')(inputs)\n",
    "encoded = BatchNormalization(name='batchnorm_1')(encoded)\n",
    "encoded = LeakyReLU(name='leaky_relu_1')(encoded)\n",
    "# Conv Block 2 -> BatchNorm->leaky Relu\n",
    "encoded = Conv2D(64, kernel_size=(3,3), strides= 2, padding='same', name='conv_2')(encoded)\n",
    "encoded = BatchNormalization(name='batchnorm_2')(encoded)\n",
    "encoded = LeakyReLU(name='leaky_relu_2')(encoded)\n",
    "# Conv Block 3 -> BatchNorm->leaky Relu\n",
    "encoded = Conv2D(64, kernel_size=(3,3), strides=2, padding='same', name='conv_3')(encoded)\n",
    "encoded = BatchNormalization(name='batchnorm_3')(encoded)\n",
    "encoded = LeakyReLU(name='leaky_relu_3')(encoded)\n",
    "#Decoder\n",
    "# DeConv Block 1-> BatchNorm->leaky Relu\n",
    "decoded = Conv2DTranspose(64, 3, strides= 1, padding='same',name='conv_transpose_1')(encoded)\n",
    "decoded = BatchNormalization(name='batchnorm_4')(decoded)\n",
    "decoded = LeakyReLU(name='leaky_relu_4')(decoded)\n",
    "# DeConv Block 2-> BatchNorm->leaky Relu\n",
    "decoded = Conv2DTranspose(64, 3, strides= 2, padding='same', name='conv_transpose_2')(decoded)\n",
    "decoded = BatchNormalization(name='batchnorm_5')(decoded)\n",
    "decoded = LeakyReLU(name='leaky_relu_5')(decoded)\n",
    "# DeConv Block 3-> BatchNorm->leaky Relu\n",
    "decoded = Conv2DTranspose(32, 3, 2, padding='same', name='conv_transpose_3')(decoded)\n",
    "decoded = BatchNormalization(name='batchnorm_6')(decoded)\n",
    "decoded = LeakyReLU(name='leaky_relu_6')(decoded)\n",
    "# output\n",
    "outputs = Conv2DTranspose(1, 3, 1,padding='same', activation='sigmoid', name='conv_transpose_4')(decoded)\n",
    "\n",
    "def SSIMLoss(y_true, y_pred):\n",
    "  return 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, 2.0))\n",
    "\n",
    "autoencoder = tf.keras.Model(inputs, outputs)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0005)\n",
    "autoencoder.compile(optimizer=optimizer, loss=SSIMLoss)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/14 [======================>.......] - ETA: 48s - loss: 0.6344 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m hist\u001b[39m=\u001b[39mautoencoder\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtrain_data,\n\u001b[0;32m      2\u001b[0m                      epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[0;32m      3\u001b[0m                      batch_size\u001b[39m=\u001b[39;49mnbatch,\n\u001b[0;32m      4\u001b[0m                      shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      5\u001b[0m                      validation_data\u001b[39m=\u001b[39;49mvalid_data)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1648\u001b[0m ):\n\u001b[0;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py:1249\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m   1247\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain_function\u001b[39m(iterator):\n\u001b[0;32m   1248\u001b[0m     \u001b[39m\"\"\"Runs a training execution with a single step.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1249\u001b[0m     \u001b[39mreturn\u001b[39;00m step_function(\u001b[39mself\u001b[39;49m, iterator)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py:1233\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m   1229\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[0;32m   1230\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m     )\n\u001b[0;32m   1232\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[1;32m-> 1233\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[0;32m   1234\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[0;32m   1235\u001b[0m     outputs,\n\u001b[0;32m   1236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[0;32m   1237\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[0;32m   1238\u001b[0m )\n\u001b[0;32m   1239\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1316\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1311\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[0;32m   1312\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[0;32m   1314\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[0;32m   1315\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m-> 1316\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2895\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   2893\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m   2894\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[1;32m-> 2895\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3696\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3694\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m   3695\u001b[0m   \u001b[39mwith\u001b[39;00m ReplicaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[1;32m-> 3696\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    594\u001b[0m   \u001b[39mwith\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mControlStatusCtx(status\u001b[39m=\u001b[39mag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mUNSPECIFIED):\n\u001b[1;32m--> 595\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py:1222\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[1;32m-> 1222\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[0;32m   1223\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[0;32m   1224\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\training.py:1027\u001b[0m, in \u001b[0;36mModel.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[0;32m   1026\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[1;32m-> 1027\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_variables, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[0;32m   1028\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:526\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[1;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminimize\u001b[39m(\u001b[39mself\u001b[39m, loss, var_list, tape\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    506\u001b[0m     \u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m \n\u001b[0;32m    508\u001b[0m \u001b[39m    This method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    524\u001b[0m \u001b[39m      None\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 526\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_gradients(loss, var_list, tape)\n\u001b[0;32m    527\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_gradients(grads_and_vars)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:259\u001b[0m, in \u001b[0;36m_BaseOptimizer.compute_gradients\u001b[1;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[39mif\u001b[39;00m callable(var_list):\n\u001b[0;32m    257\u001b[0m             var_list \u001b[39m=\u001b[39m var_list()\n\u001b[1;32m--> 259\u001b[0m grads \u001b[39m=\u001b[39m tape\u001b[39m.\u001b[39;49mgradient(loss, var_list)\n\u001b[0;32m    260\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, var_list))\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1112\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1106\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[0;32m   1107\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1108\u001b[0m           output_gradients))\n\u001b[0;32m   1109\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1110\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1112\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[0;32m   1113\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[0;32m   1114\u001b[0m     flat_targets,\n\u001b[0;32m   1115\u001b[0m     flat_sources,\n\u001b[0;32m   1116\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[0;32m   1117\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[0;32m   1118\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[0;32m   1120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[0;32m   1121\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[0;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m     target,\n\u001b[0;32m     70\u001b[0m     sources,\n\u001b[0;32m     71\u001b[0m     output_gradients,\n\u001b[0;32m     72\u001b[0m     sources_raw,\n\u001b[0;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:157\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    155\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[0;32m    158\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py:925\u001b[0m, in \u001b[0;36m_FusedBatchNormV3Grad\u001b[1;34m(op, *grad)\u001b[0m\n\u001b[0;32m    923\u001b[0m \u001b[39m@ops\u001b[39m\u001b[39m.\u001b[39mRegisterGradient(\u001b[39m\"\u001b[39m\u001b[39mFusedBatchNormV3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    924\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_FusedBatchNormV3Grad\u001b[39m(op, \u001b[39m*\u001b[39mgrad):\n\u001b[1;32m--> 925\u001b[0m   \u001b[39mreturn\u001b[39;00m _BaseFusedBatchNormGrad(op, \u001b[39m2\u001b[39;49m, \u001b[39m*\u001b[39;49mgrad)\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py:881\u001b[0m, in \u001b[0;36m_BaseFusedBatchNormGrad\u001b[1;34m(op, version, *grad)\u001b[0m\n\u001b[0;32m    879\u001b[0m   \u001b[39mif\u001b[39;00m version \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[0;32m    880\u001b[0m     args[\u001b[39m\"\u001b[39m\u001b[39mreserve_space_3\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39moutputs[\u001b[39m5\u001b[39m]\n\u001b[1;32m--> 881\u001b[0m   dx, dscale, doffset, _, _ \u001b[39m=\u001b[39m grad_fun(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs)\n\u001b[0;32m    882\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    883\u001b[0m   pop_mean \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39minputs[\u001b[39m3\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\User\\Anaconda3\\envs\\tensorflow_env\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:4661\u001b[0m, in \u001b[0;36mfused_batch_norm_grad_v3\u001b[1;34m(y_backprop, x, scale, reserve_space_1, reserve_space_2, reserve_space_3, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[0;32m   4659\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   4660\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 4661\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   4662\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mFusedBatchNormGradV3\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, y_backprop, x, scale,\n\u001b[0;32m   4663\u001b[0m       reserve_space_1, reserve_space_2, reserve_space_3, \u001b[39m\"\u001b[39;49m\u001b[39mepsilon\u001b[39;49m\u001b[39m\"\u001b[39;49m, epsilon,\n\u001b[0;32m   4664\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mdata_format\u001b[39;49m\u001b[39m\"\u001b[39;49m, data_format, \u001b[39m\"\u001b[39;49m\u001b[39mis_training\u001b[39;49m\u001b[39m\"\u001b[39;49m, is_training)\n\u001b[0;32m   4665\u001b[0m     _result \u001b[39m=\u001b[39m _FusedBatchNormGradV3Output\u001b[39m.\u001b[39m_make(_result)\n\u001b[0;32m   4666\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist=autoencoder.fit(x=train_data,\n",
    "                     epochs=10,\n",
    "                     batch_size=nbatch,\n",
    "                     shuffle=True,\n",
    "                     validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tensorflow_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0fe3e463de132c429cee0c53a81217854f4ef39ce4247c4bb179d216dafc3da0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
